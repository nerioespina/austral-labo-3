{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bc2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927a15e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>10234</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05300</td>\n",
       "      <td>0.05300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>10032</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.13628</td>\n",
       "      <td>0.13628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>10217</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03028</td>\n",
       "      <td>0.03028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>10125</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02271</td>\n",
       "      <td>0.02271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>10012</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.54452</td>\n",
       "      <td>1.54452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periodo  customer_id  product_id  plan_precios_cuidados  cust_request_qty  \\\n",
       "0   201701        10234       20524                      0                 2   \n",
       "1   201701        10032       20524                      0                 1   \n",
       "2   201701        10217       20524                      0                 1   \n",
       "3   201701        10125       20524                      0                 1   \n",
       "4   201701        10012       20524                      0                11   \n",
       "\n",
       "   cust_request_tn       tn  \n",
       "0          0.05300  0.05300  \n",
       "1          0.13628  0.13628  \n",
       "2          0.03028  0.03028  \n",
       "3          0.02271  0.02271  \n",
       "4          1.54452  1.54452  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/sell-in.txt', sep='\\t', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439fbc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>1504.68856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>1087.30855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>892.50129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>637.90002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>593.24443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id          tn\n",
       "0       20001  1504.68856\n",
       "1       20002  1087.30855\n",
       "2       20003   892.50129\n",
       "3       20004   637.90002\n",
       "4       20005   593.24443"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_201912 = df[df['periodo'] == 201912]\n",
    "df_sum_tn = df_201912.groupby('product_id', as_index=False)['tn'].sum()\n",
    "df_sum_tn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac9a8c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>20937</td>\n",
       "      <td>1.03081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id       tn\n",
       "742       20937  1.03081"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum_tn[df_sum_tn['product_id']==20937]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df83a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caract = pd.read_csv('../data/tb_productos.txt', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507bcb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>LIMPIEX</td>\n",
       "      <td>900</td>\n",
       "      <td>20280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>LIMPIEX</td>\n",
       "      <td>450</td>\n",
       "      <td>20180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>LIMPIEX</td>\n",
       "      <td>120</td>\n",
       "      <td>20332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>LIMPIEX</td>\n",
       "      <td>450</td>\n",
       "      <td>20222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>LIMPIEX</td>\n",
       "      <td>900</td>\n",
       "      <td>20288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat1         cat2     cat3    brand  sku_size  product_id\n",
       "0   HC  ROPA LAVADO  Liquido  LIMPIEX       900       20280\n",
       "1   HC  ROPA LAVADO  Liquido  LIMPIEX       450       20180\n",
       "2   HC  ROPA LAVADO  Liquido  LIMPIEX       120       20332\n",
       "3   HC  ROPA LAVADO  Liquido  LIMPIEX       450       20222\n",
       "4   HC  ROPA LAVADO  Liquido  LIMPIEX       900       20288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FECHA_CORTE = '2019-10-01'\n",
    "horizonte_prediccion = 2\n",
    "\n",
    "product_ids = df['product_id'].unique()\n",
    "df_pred_final = pd.DataFrame()\n",
    "\n",
    "for pid in product_ids:\n",
    "    df_prod = df[df['product_id'] == pid].copy()\n",
    "    if df_prod.empty:\n",
    "        continue\n",
    "\n",
    "    df_prod['unique_id'] = df_prod['product_id'].astype(str) + \"_\" + df_prod['customer_id'].astype(str)\n",
    "    df_prod['ds'] = pd.to_datetime(df_prod['periodo'], format='%Y%m')\n",
    "    df_prod['y'] = df_prod['y'].fillna(0)\n",
    "    \n",
    "    # SOLUCIÓN 1: Eliminar duplicados agregando valores por fecha\n",
    "    df_final_prod = df_prod.groupby(['unique_id', 'ds'], as_index=False)['y'].sum()\n",
    "    df_final_prod = df_final_prod.sort_values(by=['unique_id', 'ds']).reset_index(drop=True)\n",
    "    \n",
    "    # Filtrar datos de entrenamiento\n",
    "    df_entrenamiento_prod = df_final_prod[df_final_prod['ds'] <= FECHA_CORTE]\n",
    "    \n",
    "    # Verificar que no hay duplicados antes de continuar\n",
    "    duplicates = df_entrenamiento_prod.duplicated(subset=['unique_id', 'ds']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Advertencia: {duplicates} duplicados encontrados para product_id {pid}\")\n",
    "        # Eliminar duplicados manteniendo el último valor\n",
    "        df_entrenamiento_prod = df_entrenamiento_prod.drop_duplicates(\n",
    "            subset=['unique_id', 'ds'], keep='last'\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        maes = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(df_entrenamiento_prod):\n",
    "            train_data = df_entrenamiento_prod.iloc[train_idx].copy()\n",
    "            val_data = df_entrenamiento_prod.iloc[val_idx].copy()\n",
    "            \n",
    "            # Verificar que no hay duplicados en los datos de validación\n",
    "            val_data = val_data.drop_duplicates(subset=['unique_id', 'ds'], keep='last')\n",
    "            \n",
    "            try:\n",
    "                fcst = MLForecast(\n",
    "                    models=LGBMRegressor(**params),\n",
    "                    freq='MS',\n",
    "                    lags=list(range(1, 25)),\n",
    "                    date_features=['month', 'year'],\n",
    "                )\n",
    "                fcst.fit(train_data, static_features=[])\n",
    "                \n",
    "                # El horizonte es el número de fechas únicas en el conjunto de validación\n",
    "                h = val_data['ds'].nunique()\n",
    "                preds = fcst.predict(h=h)\n",
    "                \n",
    "                # Asegurar que no hay duplicados en las predicciones\n",
    "                preds = preds.drop_duplicates(subset=['unique_id', 'ds'], keep='last')\n",
    "                \n",
    "                # Merge con datos de validación para alinear predicciones y valores reales\n",
    "                comparison_df = pd.merge(\n",
    "                    val_data, \n",
    "                    preds, \n",
    "                    on=['unique_id', 'ds'], \n",
    "                    how='inner'  # Solo mantener registros que coincidan\n",
    "                )\n",
    "                \n",
    "                if len(comparison_df) > 0:\n",
    "                    # Calcular MAE en los datos alineados\n",
    "                    maes.append(mean_absolute_error(comparison_df['y'], comparison_df['LGBMRegressor']))\n",
    "                else:\n",
    "                    # Si no hay datos para comparar, asignar un MAE alto\n",
    "                    maes.append(1000)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error en validación cruzada: {e}\")\n",
    "                maes.append(1000)  # Penalizar parámetros que causan errores\n",
    "\n",
    "        return np.mean(maes)\n",
    "\n",
    "    try:\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        best_params = study.best_params\n",
    "        best_params['random_state'] = 42\n",
    "\n",
    "        # Entrenar modelo final con mejores parámetros\n",
    "        fcst_prod = MLForecast(\n",
    "            models=LGBMRegressor(**best_params),\n",
    "            freq='MS',\n",
    "            lags=list(range(1, 25)),\n",
    "            date_features=['month', 'year'],\n",
    "        )\n",
    "        fcst_prod.fit(df_entrenamiento_prod, static_features=[])\n",
    "\n",
    "        pred_prod = fcst_prod.predict(h=horizonte_prediccion)\n",
    "        pred_prod['product_id'] = pid\n",
    "\n",
    "        # Filtrar predicciones para diciembre 2019\n",
    "        pred_prod_201912 = pred_prod[pred_prod['ds'] == '2019-12-01'].copy()\n",
    "        \n",
    "        if not pred_prod_201912.empty:\n",
    "            pred_prod_201912['customer_id'] = pred_prod_201912['unique_id'].str.split('_').str[1].astype(int)\n",
    "            pred_prod_201912.rename(columns={'LGBMRegressor': 'tn'}, inplace=True)\n",
    "\n",
    "            df_pred_final = pd.concat([\n",
    "                df_pred_final, \n",
    "                pred_prod_201912[['product_id', 'customer_id', 'tn']]\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No se encontraron predicciones para diciembre 2019 en product_id {pid}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando product_id {pid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Resumen final\n",
    "if not df_pred_final.empty:\n",
    "    df_pred_sum = df_pred_final.groupby('product_id', as_index=False)['tn'].sum()\n",
    "    print(df_pred_sum)\n",
    "else:\n",
    "    print(\"No se generaron predicciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizado\n",
    "\n",
    "FECHA_CORTE = '2019-10-01'\n",
    "horizonte_prediccion = 2\n",
    "\n",
    "product_ids = df['product_id'].unique()\n",
    "df_pred_final = pd.DataFrame()\n",
    "\n",
    "for pid in product_ids:\n",
    "    df_prod = df[df['product_id'] == pid].copy()\n",
    "    if df_prod.empty:\n",
    "        continue\n",
    "\n",
    "    df_prod['unique_id'] = df_prod['product_id'].astype(str) + \"_\" + df_prod['customer_id'].astype(str)\n",
    "    df_prod['ds'] = pd.to_datetime(df_prod['periodo'], format='%Y%m')\n",
    "    df_prod['y'] = df_prod['y'].fillna(0)\n",
    "    \n",
    "    # SOLUCIÓN 1: Eliminar duplicados agregando valores por fecha\n",
    "    df_final_prod = df_prod.groupby(['unique_id', 'ds'], as_index=False)['y'].sum()\n",
    "    df_final_prod = df_final_prod.sort_values(by=['unique_id', 'ds']).reset_index(drop=True)\n",
    "    \n",
    "    # Filtrar datos de entrenamiento\n",
    "    df_entrenamiento_prod = df_final_prod[df_final_prod['ds'] <= FECHA_CORTE]\n",
    "    \n",
    "    # Verificar que no hay duplicados antes de continuar\n",
    "    duplicates = df_entrenamiento_prod.duplicated(subset=['unique_id', 'ds']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Advertencia: {duplicates} duplicados encontrados para product_id {pid}\")\n",
    "        # Eliminar duplicados manteniendo el último valor\n",
    "        df_entrenamiento_prod = df_entrenamiento_prod.drop_duplicates(\n",
    "            subset=['unique_id', 'ds'], keep='last'\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        maes = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(df_entrenamiento_prod):\n",
    "            train_data = df_entrenamiento_prod.iloc[train_idx].copy()\n",
    "            val_data = df_entrenamiento_prod.iloc[val_idx].copy()\n",
    "            \n",
    "            # Verificar que no hay duplicados en los datos de validación\n",
    "            val_data = val_data.drop_duplicates(subset=['unique_id', 'ds'], keep='last')\n",
    "            \n",
    "            try:\n",
    "                fcst = MLForecast(\n",
    "                    models=LGBMRegressor(**params),\n",
    "                    freq='MS',\n",
    "                    lags=list(range(1, 25)),\n",
    "                    date_features=['month', 'year'],\n",
    "                )\n",
    "                fcst.fit(train_data, static_features=[])\n",
    "                \n",
    "                # El horizonte es el número de fechas únicas en el conjunto de validación\n",
    "                h = val_data['ds'].nunique()\n",
    "                preds = fcst.predict(h=h)\n",
    "                \n",
    "                # Asegurar que no hay duplicados en las predicciones\n",
    "                preds = preds.drop_duplicates(subset=['unique_id', 'ds'], keep='last')\n",
    "                \n",
    "                # Merge con datos de validación para alinear predicciones y valores reales\n",
    "                comparison_df = pd.merge(\n",
    "                    val_data, \n",
    "                    preds, \n",
    "                    on=['unique_id', 'ds'], \n",
    "                    how='inner'  # Solo mantener registros que coincidan\n",
    "                )\n",
    "                \n",
    "                if len(comparison_df) > 0:\n",
    "                    # Calcular MAE en los datos alineados\n",
    "                    maes.append(mean_absolute_error(comparison_df['y'], comparison_df['LGBMRegressor']))\n",
    "                else:\n",
    "                    # Si no hay datos para comparar, asignar un MAE alto\n",
    "                    maes.append(1000)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error en validación cruzada: {e}\")\n",
    "                maes.append(1000)  # Penalizar parámetros que causan errores\n",
    "\n",
    "        return np.mean(maes)\n",
    "\n",
    "    try:\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "        best_params = study.best_params\n",
    "        best_params['random_state'] = 42\n",
    "\n",
    "        # Entrenar modelo final con mejores parámetros\n",
    "        fcst_prod = MLForecast(\n",
    "            models=LGBMRegressor(**best_params),\n",
    "            freq='MS',\n",
    "            lags=list(range(1, 25)),\n",
    "            date_features=['month', 'year'],\n",
    "        )\n",
    "        fcst_prod.fit(df_entrenamiento_prod, static_features=[])\n",
    "\n",
    "        pred_prod = fcst_prod.predict(h=horizonte_prediccion)\n",
    "        pred_prod['product_id'] = pid\n",
    "\n",
    "        # Filtrar predicciones para diciembre 2019\n",
    "        pred_prod_201912 = pred_prod[pred_prod['ds'] == '2019-12-01'].copy()\n",
    "        \n",
    "        if not pred_prod_201912.empty:\n",
    "            pred_prod_201912['customer_id'] = pred_prod_201912['unique_id'].str.split('_').str[1].astype(int)\n",
    "            pred_prod_201912.rename(columns={'LGBMRegressor': 'tn'}, inplace=True)\n",
    "\n",
    "            df_pred_final = pd.concat([\n",
    "                df_pred_final, \n",
    "                pred_prod_201912[['product_id', 'customer_id', 'tn']]\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No se encontraron predicciones para diciembre 2019 en product_id {pid}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando product_id {pid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Resumen final\n",
    "if not df_pred_final.empty:\n",
    "    df_pred_sum = df_pred_final.groupby('product_id', as_index=False)['tn'].sum()\n",
    "    print(df_pred_sum)\n",
    "else:\n",
    "    print(\"No se generaron predicciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FECHA_CORTE = '2019-12-01'\n",
    "horizonte_prediccion = 2  # enero y febrero 2020\n",
    "\n",
    "product_ids = df['product_id'].unique()\n",
    "df_pred_final = pd.DataFrame()\n",
    "\n",
    "for pid in product_ids:\n",
    "    df_prod = df[df['product_id'] == pid].copy()\n",
    "    if df_prod.empty:\n",
    "        continue\n",
    "\n",
    "    df_prod['unique_id'] = df_prod['product_id'].astype(str) + \"_\" + df_prod['customer_id'].astype(str)\n",
    "    df_prod['ds'] = pd.to_datetime(df_prod['periodo'], format='%Y%m')\n",
    "    df_prod['y'] = df_prod['y'].fillna(0)\n",
    "    df_final_prod = df_prod[['unique_id', 'ds', 'y']].sort_values(by=['unique_id', 'ds']).reset_index(drop=True)\n",
    "    df_final_prod = df_final_prod.loc[:, ~df_final_prod.columns.duplicated()]\n",
    "    df_entrenamiento_prod = df_final_prod[df_final_prod['ds'] <= FECHA_CORTE]\n",
    "\n",
    "    fcst_prod = MLForecast(\n",
    "        models=LGBMRegressor(random_state=42, n_estimators=100),\n",
    "        freq='MS',\n",
    "        lags=list(range(1, 25)),\n",
    "        date_features=['month', 'year'],\n",
    "    )\n",
    "    fcst_prod.fit(df_entrenamiento_prod, static_features=[])\n",
    "\n",
    "    pred_prod = fcst_prod.predict(h=horizonte_prediccion)\n",
    "    pred_prod['product_id'] = pid\n",
    "\n",
    "    pred_prod_202002 = pred_prod[pred_prod['ds'] == '2020-02-01'].copy()\n",
    "    pred_prod_202002['customer_id'] = pred_prod_202002['unique_id'].str.split('_').str[1].astype(int)\n",
    "    pred_prod_202002.rename(columns={'LGBMRegressor': 'tn'}, inplace=True)\n",
    "\n",
    "    df_pred_final = pd.concat([df_pred_final, pred_prod_202002[['product_id', 'customer_id', 'tn']]], ignore_index=True)\n",
    "\n",
    "df_pred_sum = df_pred_final.groupby('product_id', as_index=False)['tn'].sum()\n",
    "print(df_pred_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
