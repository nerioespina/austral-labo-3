{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f255cb3c",
   "metadata": {},
   "source": [
    "# Predicción de despacho de productos\n",
    "## Alternativa 1: Modelo por cada producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d301a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pmdarima as pm\n",
    "from skopt.space import Integer, Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43fefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = pd.read_csv('../data/pivot_df_interpolado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043eccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262805, 38)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f51d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>201701</th>\n",
       "      <th>201702</th>\n",
       "      <th>201703</th>\n",
       "      <th>201704</th>\n",
       "      <th>201705</th>\n",
       "      <th>201706</th>\n",
       "      <th>201707</th>\n",
       "      <th>201708</th>\n",
       "      <th>...</th>\n",
       "      <th>201903</th>\n",
       "      <th>201904</th>\n",
       "      <th>201905</th>\n",
       "      <th>201906</th>\n",
       "      <th>201907</th>\n",
       "      <th>201908</th>\n",
       "      <th>201909</th>\n",
       "      <th>201910</th>\n",
       "      <th>201911</th>\n",
       "      <th>201912</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>10001</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>198.84365</td>\n",
       "      <td>92.46537</td>\n",
       "      <td>13.29728</td>\n",
       "      <td>101.00563</td>\n",
       "      <td>128.04792</td>\n",
       "      <td>101.20711</td>\n",
       "      <td>43.33930</td>\n",
       "      <td>...</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>109.05244</td>\n",
       "      <td>176.02980</td>\n",
       "      <td>236.65556</td>\n",
       "      <td>180.21938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>10002</td>\n",
       "      <td>35.72806</td>\n",
       "      <td>6.79415</td>\n",
       "      <td>29.94128</td>\n",
       "      <td>22.81133</td>\n",
       "      <td>31.22847</td>\n",
       "      <td>47.57025</td>\n",
       "      <td>21.84874</td>\n",
       "      <td>17.08052</td>\n",
       "      <td>...</td>\n",
       "      <td>31.97079</td>\n",
       "      <td>55.41679</td>\n",
       "      <td>30.87299</td>\n",
       "      <td>144.07021</td>\n",
       "      <td>37.14616</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>72.08551</td>\n",
       "      <td>17.40806</td>\n",
       "      <td>45.61495</td>\n",
       "      <td>113.33165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20001</td>\n",
       "      <td>10003</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>20.48319</td>\n",
       "      <td>137.87537</td>\n",
       "      <td>68.89292</td>\n",
       "      <td>135.12190</td>\n",
       "      <td>171.01785</td>\n",
       "      <td>64.66196</td>\n",
       "      <td>83.63410</td>\n",
       "      <td>...</td>\n",
       "      <td>170.89924</td>\n",
       "      <td>230.00152</td>\n",
       "      <td>1.84835</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>138.23391</td>\n",
       "      <td>162.07198</td>\n",
       "      <td>233.20532</td>\n",
       "      <td>76.00625</td>\n",
       "      <td>86.14415</td>\n",
       "      <td>102.27517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001</td>\n",
       "      <td>10004</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>104.03894</td>\n",
       "      <td>295.43924</td>\n",
       "      <td>247.65632</td>\n",
       "      <td>188.37819</td>\n",
       "      <td>195.02683</td>\n",
       "      <td>379.44270</td>\n",
       "      <td>237.16848</td>\n",
       "      <td>...</td>\n",
       "      <td>102.64484</td>\n",
       "      <td>91.67799</td>\n",
       "      <td>389.02653</td>\n",
       "      <td>66.71971</td>\n",
       "      <td>228.62366</td>\n",
       "      <td>96.11402</td>\n",
       "      <td>288.34205</td>\n",
       "      <td>324.96172</td>\n",
       "      <td>195.67828</td>\n",
       "      <td>34.64810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20001</td>\n",
       "      <td>10005</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>5.17117</td>\n",
       "      <td>5.17117</td>\n",
       "      <td>0.86186</td>\n",
       "      <td>37.95546</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>43.35049</td>\n",
       "      <td>67.53856</td>\n",
       "      <td>...</td>\n",
       "      <td>6.90049</td>\n",
       "      <td>22.18016</td>\n",
       "      <td>15.89578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.25595</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.80400</td>\n",
       "      <td>17.13921</td>\n",
       "      <td>12.22149</td>\n",
       "      <td>19.60368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  customer_id     201701     201702     201703     201704  \\\n",
       "0       20001        10001   99.43861  198.84365   92.46537   13.29728   \n",
       "1       20001        10002   35.72806    6.79415   29.94128   22.81133   \n",
       "2       20001        10003  143.49426   20.48319  137.87537   68.89292   \n",
       "3       20001        10004  184.72927  104.03894  295.43924  247.65632   \n",
       "4       20001        10005   19.08407    5.17117    5.17117    0.86186   \n",
       "\n",
       "      201705     201706     201707     201708  ...     201903     201904  \\\n",
       "0  101.00563  128.04792  101.20711   43.33930  ...  130.54927  364.37071   \n",
       "1   31.22847   47.57025   21.84874   17.08052  ...   31.97079   55.41679   \n",
       "2  135.12190  171.01785   64.66196   83.63410  ...  170.89924  230.00152   \n",
       "3  188.37819  195.02683  379.44270  237.16848  ...  102.64484   91.67799   \n",
       "4   37.95546   19.08407   43.35049   67.53856  ...    6.90049   22.18016   \n",
       "\n",
       "      201905     201906     201907     201908     201909     201910  \\\n",
       "0  439.90647   65.92436  144.78714   33.63991  109.05244  176.02980   \n",
       "1   30.87299  144.07021   37.14616    0.00000   72.08551   17.40806   \n",
       "2    1.84835    0.00000  138.23391  162.07198  233.20532   76.00625   \n",
       "3  389.02653   66.71971  228.62366   96.11402  288.34205  324.96172   \n",
       "4   15.89578    0.00000    8.25595    0.00000   12.80400   17.13921   \n",
       "\n",
       "      201911     201912  \n",
       "0  236.65556  180.21938  \n",
       "1   45.61495  113.33165  \n",
       "2   86.14415  102.27517  \n",
       "3  195.67828   34.64810  \n",
       "4   12.22149   19.60368  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where all columns except 'product_id' and 'customer_id' are null\n",
    "cols_to_check = pivot_df.columns.difference(['product_id', 'customer_id'])\n",
    "pivot_df = pivot_df.dropna(subset=cols_to_check, how='all')\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6464ba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262805, 38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85f3cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>201701</th>\n",
       "      <th>201702</th>\n",
       "      <th>201703</th>\n",
       "      <th>201704</th>\n",
       "      <th>201705</th>\n",
       "      <th>201706</th>\n",
       "      <th>201707</th>\n",
       "      <th>201708</th>\n",
       "      <th>...</th>\n",
       "      <th>201903</th>\n",
       "      <th>201904</th>\n",
       "      <th>201905</th>\n",
       "      <th>201906</th>\n",
       "      <th>201907</th>\n",
       "      <th>201908</th>\n",
       "      <th>201909</th>\n",
       "      <th>201910</th>\n",
       "      <th>201911</th>\n",
       "      <th>201912</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>10001</td>\n",
       "      <td>34.011090</td>\n",
       "      <td>91.505733</td>\n",
       "      <td>34.227381</td>\n",
       "      <td>5.620967</td>\n",
       "      <td>43.017729</td>\n",
       "      <td>53.427125</td>\n",
       "      <td>42.221730</td>\n",
       "      <td>20.868623</td>\n",
       "      <td>...</td>\n",
       "      <td>75.994505</td>\n",
       "      <td>197.630988</td>\n",
       "      <td>222.727019</td>\n",
       "      <td>41.281357</td>\n",
       "      <td>84.468477</td>\n",
       "      <td>25.934477</td>\n",
       "      <td>61.055903</td>\n",
       "      <td>79.241259</td>\n",
       "      <td>129.845076</td>\n",
       "      <td>116.150285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>10002</td>\n",
       "      <td>12.111477</td>\n",
       "      <td>2.983680</td>\n",
       "      <td>10.997802</td>\n",
       "      <td>9.722515</td>\n",
       "      <td>13.219633</td>\n",
       "      <td>19.782447</td>\n",
       "      <td>9.049391</td>\n",
       "      <td>8.162839</td>\n",
       "      <td>...</td>\n",
       "      <td>18.548231</td>\n",
       "      <td>29.995683</td>\n",
       "      <td>15.568913</td>\n",
       "      <td>90.310715</td>\n",
       "      <td>21.615563</td>\n",
       "      <td>-0.073820</td>\n",
       "      <td>40.334746</td>\n",
       "      <td>7.781788</td>\n",
       "      <td>24.972975</td>\n",
       "      <td>73.018654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20001</td>\n",
       "      <td>10003</td>\n",
       "      <td>49.154605</td>\n",
       "      <td>9.293417</td>\n",
       "      <td>51.098561</td>\n",
       "      <td>29.588490</td>\n",
       "      <td>57.586964</td>\n",
       "      <td>71.391232</td>\n",
       "      <td>26.945608</td>\n",
       "      <td>40.365988</td>\n",
       "      <td>...</td>\n",
       "      <td>99.508313</td>\n",
       "      <td>124.723610</td>\n",
       "      <td>0.869164</td>\n",
       "      <td>-0.080137</td>\n",
       "      <td>80.641964</td>\n",
       "      <td>125.230207</td>\n",
       "      <td>130.647599</td>\n",
       "      <td>34.180412</td>\n",
       "      <td>47.221556</td>\n",
       "      <td>65.889035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001</td>\n",
       "      <td>10004</td>\n",
       "      <td>63.328565</td>\n",
       "      <td>47.807064</td>\n",
       "      <td>109.638275</td>\n",
       "      <td>106.654172</td>\n",
       "      <td>80.329879</td>\n",
       "      <td>81.428481</td>\n",
       "      <td>158.526102</td>\n",
       "      <td>114.656364</td>\n",
       "      <td>...</td>\n",
       "      <td>59.733293</td>\n",
       "      <td>49.670648</td>\n",
       "      <td>196.958487</td>\n",
       "      <td>41.780366</td>\n",
       "      <td>133.421669</td>\n",
       "      <td>74.235593</td>\n",
       "      <td>161.553515</td>\n",
       "      <td>146.335444</td>\n",
       "      <td>107.350521</td>\n",
       "      <td>22.280652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20001</td>\n",
       "      <td>10005</td>\n",
       "      <td>6.390337</td>\n",
       "      <td>2.235594</td>\n",
       "      <td>1.794961</td>\n",
       "      <td>0.260003</td>\n",
       "      <td>16.092371</td>\n",
       "      <td>7.873450</td>\n",
       "      <td>18.037269</td>\n",
       "      <td>32.577871</td>\n",
       "      <td>...</td>\n",
       "      <td>3.938598</td>\n",
       "      <td>11.961821</td>\n",
       "      <td>7.983592</td>\n",
       "      <td>-0.080137</td>\n",
       "      <td>4.746208</td>\n",
       "      <td>-0.073820</td>\n",
       "      <td>7.105547</td>\n",
       "      <td>7.660670</td>\n",
       "      <td>6.641573</td>\n",
       "      <td>12.579465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  customer_id     201701     201702      201703      201704  \\\n",
       "0       20001        10001  34.011090  91.505733   34.227381    5.620967   \n",
       "1       20001        10002  12.111477   2.983680   10.997802    9.722515   \n",
       "2       20001        10003  49.154605   9.293417   51.098561   29.588490   \n",
       "3       20001        10004  63.328565  47.807064  109.638275  106.654172   \n",
       "4       20001        10005   6.390337   2.235594    1.794961    0.260003   \n",
       "\n",
       "      201705     201706      201707      201708  ...     201903      201904  \\\n",
       "0  43.017729  53.427125   42.221730   20.868623  ...  75.994505  197.630988   \n",
       "1  13.219633  19.782447    9.049391    8.162839  ...  18.548231   29.995683   \n",
       "2  57.586964  71.391232   26.945608   40.365988  ...  99.508313  124.723610   \n",
       "3  80.329879  81.428481  158.526102  114.656364  ...  59.733293   49.670648   \n",
       "4  16.092371   7.873450   18.037269   32.577871  ...   3.938598   11.961821   \n",
       "\n",
       "       201905     201906      201907      201908      201909      201910  \\\n",
       "0  222.727019  41.281357   84.468477   25.934477   61.055903   79.241259   \n",
       "1   15.568913  90.310715   21.615563   -0.073820   40.334746    7.781788   \n",
       "2    0.869164  -0.080137   80.641964  125.230207  130.647599   34.180412   \n",
       "3  196.958487  41.780366  133.421669   74.235593  161.553515  146.335444   \n",
       "4    7.983592  -0.080137    4.746208   -0.073820    7.105547    7.660670   \n",
       "\n",
       "       201911      201912  \n",
       "0  129.845076  116.150285  \n",
       "1   24.972975   73.018654  \n",
       "2   47.221556   65.889035  \n",
       "3  107.350521   22.280652  \n",
       "4    6.641573   12.579465  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecciona solo las columnas de las series temporales (excluye product_id y customer_id)\n",
    "series_cols = pivot_df.columns.difference(['product_id', 'customer_id'])\n",
    "\n",
    "# Inicializa el scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajusta y transforma las columnas de series temporales\n",
    "pivot_df[series_cols] = scaler.fit_transform(pivot_df[series_cols])\n",
    "\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e29630",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     63\u001b[39m y_train_sklearn = train_data.values\n\u001b[32m     65\u001b[39m opt = BayesSearchCV(\n\u001b[32m     66\u001b[39m     SARIMAXWrapper(),\n\u001b[32m     67\u001b[39m     param_space,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sklearn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_sklearn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Mejor modelo\u001b[39;00m\n\u001b[32m     78\u001b[39m best_model = opt.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/nespina/Elements/maestriacd/austral/labo_3/.venv/lib/python3.12/site-packages/skopt/searchcv.py:542\u001b[39m, in \u001b[36mBayesSearchCV.fit\u001b[39m\u001b[34m(self, X, y, groups, callback, **fit_params)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.refit):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    537\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBayesSearchCV doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support a callable refit, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    538\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt define an implicit score to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    539\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moptimize\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_train_score:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/nespina/Elements/maestriacd/austral/labo_3/.venv/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1145\u001b[39m     estimator._validate_params()\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1148\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1149\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1150\u001b[39m     )\n\u001b[32m   1151\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/nespina/Elements/maestriacd/austral/labo_3/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:898\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, groups, **fit_params)\u001b[39m\n\u001b[32m    892\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    893\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    894\u001b[39m     )\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    902\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/nespina/Elements/maestriacd/austral/labo_3/.venv/lib/python3.12/site-packages/skopt/searchcv.py:599\u001b[39m, in \u001b[36mBayesSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter > \u001b[32m0\u001b[39m:\n\u001b[32m    596\u001b[39m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[32m    597\u001b[39m     n_points_adjusted = \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     optim_result, score_name = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_points_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m     n_iter -= n_points\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/nespina/Elements/maestriacd/austral/labo_3/.venv/lib/python3.12/site-packages/skopt/searchcv.py:448\u001b[39m, in \u001b[36mBayesSearchCV._step\u001b[39m\u001b[34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[39m\n\u001b[32m    445\u001b[39m params = optimizer.ask(n_points=n_points)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# convert parameters to python native types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m params = [[\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[32m    451\u001b[39m params_dict = [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n",
      "\u001b[31mValueError\u001b[39m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "i = 20001\n",
    "# 20001.0\t10005.0\t\n",
    "serie = pivot_df[(pivot_df['product_id'] == 20001) & (pivot_df['customer_id'] == 10001)].iloc[0, 2:]  # Exclude 'product_id' and 'customer_id' columns\n",
    "# serie = pivot_df[(pivot_df['product_id'] == 20001) & (pivot_df['customer_id'] == 10005)].iloc[0, 2:]  # Exclude 'product_id' and 'customer_id' columns\n",
    "serie = serie.dropna()\n",
    "\n",
    "# Convert index to datetime for SARIMAX\n",
    "# Assuming your index is in 'YYYYMM' format\n",
    "serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "serie = serie.sort_index()\n",
    "\n",
    "# Define training and testing periods\n",
    "train_end_date = '2019-10-31'\n",
    "test_date = '2019-12-01' # For prediction\n",
    "\n",
    "# Split data\n",
    "train_data = serie[serie.index <= train_end_date]\n",
    "test_data = serie[serie.index == test_date]\n",
    "# Wrapper para SARIMAX compatible con sklearn\n",
    "class SARIMAXWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, order=(1,1,1), seasonal_order=(1,1,0,4)):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.model_ = None\n",
    "        self.model_fit_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model_ = SARIMAX(y, order=self.order, seasonal_order=self.seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        self.model_fit_ = self.model_.fit(disp=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        dates = pd.to_datetime(np.array(X).flatten())\n",
    "        preds = []\n",
    "        for date in dates:\n",
    "            pred = self.model_fit_.predict(start=date, end=date)\n",
    "            preds.append(pred.iloc[0])\n",
    "        return np.array(preds)\n",
    "\n",
    "# Espacio de búsqueda\n",
    "param_space = {\n",
    "    'order': Categorical([\n",
    "        (0,0,0), (0,0,1), (0,1,0), (0,1,1), (0,2,1),\n",
    "        (1,0,0), (1,0,1), (1,1,0), (1,1,1), (1,1,2), (1,2,1), (1,2,2),\n",
    "        (2,0,0), (2,0,1), (2,0,2), (2,1,0), (2,1,1), (2,1,2), (2,2,1), (2,2,2),\n",
    "        (3,0,0), (3,0,1), (3,1,0), (3,1,1), (3,1,2), (3,2,1), (3,2,2)\n",
    "    ]),\n",
    "    'seasonal_order': Categorical([\n",
    "        (0,0,0,4), (0,1,0,4), (1,0,0,4), (1,1,0,4), (1,1,1,4), (2,1,0,4), (0,1,1,4), (1,0,1,4), (2,0,1,4),\n",
    "        (1,2,0,4), (2,2,0,4), (2,2,1,4), (1,2,1,4), (2,1,1,4), (2,0,2,4), (1,0,2,4), (0,2,1,4), (0,2,0,4),\n",
    "        (0,0,0,12), (0,1,0,12), (1,0,0,12), (1,1,0,12), (1,1,1,12), (2,1,0,12), (0,1,1,12), (1,0,1,12), (2,0,1,12),\n",
    "        (1,2,0,12), (2,2,0,12), (2,2,1,12), (1,2,1,12), (2,1,1,12), (2,0,2,12), (1,0,2,12), (0,2,1,12), (0,2,0,12)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Prepara los datos para sklearn (X: fechas, y: valores)\n",
    "X_train_sklearn = train_data.index.values.reshape(-1, 1)\n",
    "y_train_sklearn = train_data.values\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    SARIMAXWrapper(),\n",
    "    param_space,\n",
    "    n_iter=20,\n",
    "    cv=[(np.arange(len(X_train_sklearn)), np.arange(len(X_train_sklearn)))],  # No split, solo fit\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_sklearn, y_train_sklearn)\n",
    "\n",
    "# Mejor modelo\n",
    "best_model = opt.best_estimator_\n",
    "best_params = (best_model.order, best_model.seasonal_order)\n",
    "\n",
    "# Predicción para diciembre 2019\n",
    "pred_201912 = best_model.predict(np.array([pd.to_datetime(test_date)]))[0]\n",
    "\n",
    "print(f\"Mejor combinación: order={best_params[0]}, seasonal_order={best_params[1]}\")\n",
    "print(f\"Predicción para product_id={i} en 2019-12: {pred_201912:.5f}\")\n",
    "if not test_data.empty:\n",
    "    print(f\"Valor real: {test_data.iloc[0]:.5f}\")\n",
    "else:\n",
    "    print(\"No hay valor real disponible para comparación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20001\n",
    "# 20001.0\t10005.0\t\n",
    "# serie = pivot_df_interpolado[(pivot_df_interpolado['product_id'] == 20001) & (pivot_df_interpolado['customer_id'] == 10001)].iloc[0, 2:]  # Exclude 'product_id' and 'customer_id' columns\n",
    "serie = pivot_df_interpolado[(pivot_df_interpolado['product_id'] == 20001) & (pivot_df_interpolado['customer_id'] == 10005)].iloc[0, 2:]  # Exclude 'product_id' and 'customer_id' columns\n",
    "serie = serie.dropna()\n",
    "\n",
    "# Convert index to datetime for SARIMAX\n",
    "# Assuming your index is in 'YYYYMM' format\n",
    "serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "serie = serie.sort_index()\n",
    "\n",
    "# Define training and testing periods\n",
    "train_end_date = '2019-10-31'\n",
    "test_date = '2019-12-01' # For prediction\n",
    "\n",
    "# Split data\n",
    "train_data = serie[serie.index <= train_end_date]\n",
    "test_data = serie[serie.index == test_date]\n",
    "\n",
    "\n",
    "\n",
    "param_space = {\n",
    "    'order': Categorical([\n",
    "        (1,1,1), (2,1,1), (1,1,2), (2,1,2), (0,1,1), (1,0,1)\n",
    "    ]),\n",
    "    'seasonal_order': Categorical([\n",
    "        (1,1,0,4), (1,1,1,4), (2,1,0,4), (0,1,1,4), (1,0,1,4),\n",
    "        (1,1,0,12), (1,1,1,12), (2,1,0,12)\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and fit the SARIMAX model\n",
    "# 'enforce_stationarity=False' and 'enforce_invertibility=False' can help with convergence\n",
    "model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "model_fit = model.fit(disp=False) # disp=False to suppress optimization output\n",
    "\n",
    "# Make prediction for December 2019\n",
    "start_index = pd.to_datetime(test_date)\n",
    "end_index = pd.to_datetime(test_date)\n",
    "\n",
    "# Predict using the fitted model\n",
    "pred_201912 = model_fit.predict(start=start_index, end=end_index)\n",
    "\n",
    "print(f\"Predicción para product_id={i} en 2019-12: {pred_201912.iloc[0]:.5f}\")\n",
    "\n",
    "if not test_data.empty:\n",
    "    print(f\"Valor real: {test_data.iloc[0]:.5f}\")\n",
    "    # Optional: Calculate and print RMSE for evaluation\n",
    "    # rmse = np.sqrt(mean_squared_error(test_data, pred_201912))\n",
    "    # print(f\"RMSE: {rmse:.5f}\")\n",
    "else:\n",
    "    print(\"No hay valor real disponible para comparación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780abdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_2500 = pivot_df_interpolado.head(2500).copy()\n",
    "df_first_2500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (1,1,1)\n",
    "seasonal_order = (2,1,0,4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "\n",
    "for idx, row in df_first_2500.iterrows():\n",
    "    print(f\"Processing index: {idx}\")\n",
    "    product_id = row['product_id']\n",
    "    customer_id = row['customer_id']\n",
    "    serie = row[2:]\n",
    "    serie = serie.dropna()\n",
    "    if len(serie) < 8 or '201912' not in serie.index.astype(str):\n",
    "        continue\n",
    "\n",
    "    # Convertir el índice a fechas\n",
    "    serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "    serie = serie.sort_index()\n",
    "\n",
    "    # Definir periodos de train y test\n",
    "    train_end_date = '2019-10-31'\n",
    "    test_date = '2019-12-01'\n",
    "\n",
    "    train_data = serie[serie.index <= train_end_date]\n",
    "    test_data = serie[serie.index == test_date]\n",
    "\n",
    "    if len(train_data) < 8 or test_data.empty:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        model_fit = model.fit(disp=False)\n",
    "        pred_201912 = model_fit.predict(start=pd.to_datetime(test_date), end=pd.to_datetime(test_date)).iloc[0]\n",
    "        real_201912 = test_data.iloc[0]\n",
    "        resultados.append({\n",
    "            'product_id': product_id,\n",
    "            'customer_id': customer_id,\n",
    "            'sarimax_pred_201912': pred_201912,\n",
    "            'real_201912': real_201912\n",
    "        })\n",
    "    except Exception:\n",
    "\n",
    "        continue\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7342097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula el error porcentual absoluto medio (MAPE) en sarimax_pred_201912\n",
    "mape_df_resultados = np.mean(np.abs((df_resultados['real_201912'] - df_resultados['sarimax_pred_201912']) / df_resultados['real_201912'])) * 100\n",
    "print(f\"Error porcentual absoluto medio (MAPE) en df_resultados: {mape_df_resultados:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Wrapper para SARIMAX compatible con sklearn\n",
    "class SARIMAXWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, order=(1,1,1), seasonal_order=(1,1,0,4)):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.model_ = None\n",
    "        self.model_fit_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # SARIMAX espera un índice de fechas\n",
    "        self.model_ = SARIMAX(y, order=self.order, seasonal_order=self.seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        self.model_fit_ = self.model_.fit(disp=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X can be a 2D array (n_samples, 1) or 1D array of timestamps\n",
    "        # Convert to 1D array of pandas Timestamps\n",
    "        dates = pd.to_datetime(np.array(X).flatten())\n",
    "        preds = []\n",
    "        for date in dates:\n",
    "            pred = self.model_fit_.predict(start=date, end=date)\n",
    "            preds.append(pred.iloc[0])\n",
    "        return np.array(preds)\n",
    "\n",
    "# Espacio de búsqueda\n",
    "param_space = {\n",
    "    'order': Categorical([(1,1,1), (2,1,1), (1,1,2)]),\n",
    "    'seasonal_order': Categorical([(1,1,0,4), (1,1,1,4), (2,1,0,4)])\n",
    "}\n",
    "\n",
    "# Prepara los datos para sklearn (X: fechas, y: valores)\n",
    "X_train_sklearn = train_data.index.values.reshape(-1, 1)\n",
    "y_train_sklearn = train_data.values\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    SARIMAXWrapper(),\n",
    "    param_space,\n",
    "    n_iter=10,\n",
    "    cv=[(np.arange(len(X_train_sklearn)), np.arange(len(X_train_sklearn)))],  # No split, solo fit\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train_sklearn, y_train_sklearn)\n",
    "\n",
    "# Mejor modelo\n",
    "best_model = opt.best_estimator_\n",
    "\n",
    "# Predicción para diciembre 2019\n",
    "pred_201912 = best_model.predict(np.array([start_index]))[0]\n",
    "\n",
    "print(f\"Predicción optimizada para product_id={i} en 2019-12: {pred_201912:.5f}\")\n",
    "if not test_data.empty:\n",
    "    print(f\"Valor real: {test_data.iloc[0]:.5f}\")\n",
    "else:\n",
    "    print(\"No hay valor real disponible para comparación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_preds = []\n",
    "sarimax_real = []\n",
    "product_ids = []\n",
    "\n",
    "for prod_id in pivot_df['product_id']:\n",
    "    serie = pivot_df[pivot_df['product_id'] == prod_id].iloc[0, 1:]\n",
    "    serie = serie.dropna()\n",
    "    if len(serie) < 8 or 201912 not in serie.index:\n",
    "        continue  # Necesitamos suficientes datos y valor real en 201912\n",
    "\n",
    "    # Convertir el índice a fechas para SARIMAX\n",
    "    serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "\n",
    "    # Entrenar hasta octubre 2019\n",
    "    train_data = serie[serie.index <= '2019-10-31']\n",
    "    test_data = serie[serie.index == '2019-12-01']\n",
    "\n",
    "    try:\n",
    "        model = SARIMAX(train_data, order=(1,1,1), seasonal_order=(1,1,0,4),\n",
    "                        enforce_stationarity=False, enforce_invertibility=False)\n",
    "        model_fit = model.fit(disp=False)\n",
    "        pred = model_fit.predict(start=pd.to_datetime('2019-12-01'), end=pd.to_datetime('2019-12-01'))\n",
    "        sarimax_preds.append(pred.iloc[0])\n",
    "        sarimax_real.append(test_data.iloc[0])\n",
    "        product_ids.append(prod_id)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "df_sarimax_pred = pd.DataFrame({\n",
    "    'product_id': product_ids,\n",
    "    'sarimax_pred_201912': sarimax_preds,\n",
    "    'real_201912': sarimax_real\n",
    "})\n",
    "\n",
    "df_sarimax_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula el error porcentual absoluto medio (MAPE) entre las predicciones SARIMAX y los valores reales\n",
    "mape_sarimax = np.mean(np.abs((df_sarimax_pred['real_201912'] - df_sarimax_pred['sarimax_pred_201912']) / df_sarimax_pred['real_201912'])) * 100\n",
    "print(f\"Error porcentual absoluto medio (MAPE) SARIMAX: {mape_sarimax:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# Selecciona el producto i (por ejemplo, product_id = 20001)\n",
    "i = 20003\n",
    "serie = pivot_df[pivot_df['product_id'] == i].iloc[0, 1:]  # Excluye la columna product_id\n",
    "serie = serie.dropna()\n",
    "\n",
    "# Prepara los datos: X = mes (como entero), y = tn\n",
    "X = np.array(serie.index.astype(int)).reshape(-1, 1)\n",
    "y = serie.values\n",
    "\n",
    "# Train: hasta octubre 2019 (201910)\n",
    "train_mask = X.flatten() <= 201910\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "\n",
    "# Test: diciembre 2019 (201912)\n",
    "test_mask = X.flatten() == 201912\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "# Modelo simple: regresión lineal sobre el tiempo\n",
    "# model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "# model = RandomForestRegressor(random_state=42)\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=250, max_depth=5, min_samples_split=8, min_samples_leaf=1, max_features='sqrt', bootstrap=True)\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=250, max_depth=5, min_samples_split=8, min_samples_leaf=1, max_features='sqrt', bootstrap=True)\n",
    "\n",
    "# model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción para diciembre 2019 (201912)\n",
    "pred_201912 = model.predict(np.array([[201912]]))[0]\n",
    "\n",
    "print(f\"Predicción para product_id={i} en 2019-12: {pred_201912:.5f}\")\n",
    "if len(y_test) > 0:\n",
    "    print(f\"Valor real: {y_test[0]:.5f}\")\n",
    "else:\n",
    "    print(\"No hay valor real disponible para comparación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Integer, Categorical\n",
    "from skopt import BayesSearchCV\n",
    "iter = 0\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_space = {\n",
    "    'n_estimators': Categorical([50, 100, 150, 200, 250, 300]),#Integer(50, 400),\n",
    "    'max_depth': Categorical([1, 5, 10, 20]),\n",
    "    'min_samples_split':  Categorical([2, 4, 6, 8, 10]),\n",
    "    'min_samples_leaf':  Categorical([1, 3, 5, 7]),\n",
    "    'max_features': Categorical(['sqrt', 'log2']),\n",
    "    'bootstrap': Categorical([True])\n",
    "}\n",
    "\n",
    "\n",
    "# Optimización bayesiana con logging\n",
    "opt = BayesSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_space,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "model = opt.best_estimator_\n",
    "\n",
    "# Predicción para diciembre 2019 (201912)\n",
    "pred_201912 = model.predict(np.array([[201912]]))[0]\n",
    "\n",
    "print(f\"Predicción para product_id={i} en 2019-12: {pred_201912:.5f}\")\n",
    "if len(y_test) > 0:\n",
    "    print(f\"Valor real: {y_test[0]:.5f}\")\n",
    "else:\n",
    "    print(\"No hay valor real disponible para comparación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario para almacenar las predicciones\n",
    "predicciones = {}\n",
    "\n",
    "for prod_id in pivot_df['product_id']:\n",
    "    serie = pivot_df[pivot_df['product_id'] == prod_id].iloc[0, 1:]  # Excluye la columna product_id\n",
    "    serie = serie.dropna()\n",
    "    if len(serie) < 2:\n",
    "        continue  # No se puede ajustar un modelo con menos de 2 puntos\n",
    "\n",
    "    X_prod = np.array(serie.index.astype(int)).reshape(-1, 1)\n",
    "    y_prod = serie.values\n",
    "\n",
    "    # Train: hasta octubre 2019 (201910)\n",
    "    train_mask = X_prod.flatten() <= 201910\n",
    "    X_train_prod, y_train_prod = X_prod[train_mask], y_prod[train_mask]\n",
    "\n",
    "    # Modelo simple: regresión lineal sobre el tiempo\n",
    "    if len(X_train_prod) < 2:\n",
    "        continue  # No se puede ajustar un modelo con menos de 2 puntos\n",
    "\n",
    "    # model_prod = LinearRegression()\n",
    "    model_prod = RandomForestRegressor(random_state=42, n_estimators=250, max_depth=5, min_samples_split=8, min_samples_leaf=1, max_features='sqrt', bootstrap=True)\n",
    "    model_prod.fit(X_train_prod, y_train_prod)\n",
    "\n",
    "    # Predicción para diciembre 2019 (201912)\n",
    "    pred_201912 = model_prod.predict(np.array([[201912]]))[0]\n",
    "    predicciones[prod_id] = pred_201912\n",
    "\n",
    "# Convertimos el diccionario a DataFrame para visualizar\n",
    "df_predicciones = pd.DataFrame(list(predicciones.items()), columns=['product_id', 'pred_201912'])\n",
    "df_predicciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calcula el accuracy de las predicciones usando el error absoluto medio (MAE) y el error cuadrático medio (RMSE)\n",
    "# Solo para productos que tienen valor real en 201912\n",
    "\n",
    "# Extraer los valores reales de 201912 para cada producto\n",
    "valores_reales = df[df['periodo'] == 201912][['product_id', 'tn']]\n",
    "\n",
    "# Unir con las predicciones\n",
    "df_eval = pd.merge(df_predicciones, valores_reales, on='product_id', how='inner')\n",
    "\n",
    "# Calcular métricas\n",
    "\n",
    "mae = mean_absolute_error(df_eval['tn'], df_eval['pred_201912'])\n",
    "rmse = mean_squared_error(df_eval['tn'], df_eval['pred_201912'], squared=False)\n",
    "\n",
    "print(f\"MAE (Error absoluto medio): {mae:.4f}\")\n",
    "print(f\"RMSE (Raíz del error cuadrático medio): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c23135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e871fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = np.mean(np.abs((df_eval['tn'] - df_eval['pred_201912']) / df_eval['tn'])) * 100\n",
    "print(f\"MAPE (Error porcentual absoluto medio): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desvío promedio porcentual de las predicciones respecto a los valores reales\n",
    "desvio_promedio = np.mean(np.abs(df_eval['pred_201912'] - df_eval['tn']) / df_eval['tn']) * 100\n",
    "print(f\"Desvío promedio porcentual: {desvio_promedio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Selecciona la serie temporal del producto 20001\n",
    "serie_20001 = pivot_df[pivot_df['product_id'] == 20001].iloc[0, 1:]  # Excluye la columna product_id\n",
    "serie_20001 = serie_20001.dropna()\n",
    "\n",
    "# Convierte el índice a periodo de fecha para la serie temporal\n",
    "serie_20001.index = pd.to_datetime(serie_20001.index.astype(str), format='%Y%m')\n",
    "\n",
    "# Ajusta el modelo SARIMAX considerando estacionalidad trimestral (4 meses por estación)\n",
    "model_sarimax = SARIMAX(serie_20001, order=(1,1,1), seasonal_order=(1,1,1,4))\n",
    "result_sarimax = model_sarimax.fit(disp=False)\n",
    "\n",
    "# Predicción para diciembre 2019\n",
    "pred_sarimax = result_sarimax.get_prediction(start=pd.to_datetime('2019-12-01'), end=pd.to_datetime('2019-12-01'))\n",
    "pred_value = pred_sarimax.predicted_mean.iloc[0]\n",
    "\n",
    "print(f\"Predicción SARIMAX para product_id=20001 en 2019-12: {pred_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "sarimax_preds = []\n",
    "sarimax_real = []\n",
    "\n",
    "for prod_id in pivot_df['product_id']:\n",
    "    serie = pivot_df[pivot_df['product_id'] == prod_id].iloc[0, 1:]\n",
    "    serie = serie.dropna()\n",
    "    if len(serie) < 8 or 201912 not in serie.index:\n",
    "        continue  # Necesitamos suficientes datos y valor real en 201912\n",
    "\n",
    "    # Convertir el índice a fechas para SARIMAX\n",
    "    serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "\n",
    "    # SARIMAX simple, estacionalidad trimestral (4)\n",
    "    try:\n",
    "        model = SARIMAX(serie.iloc[:-1], order=(1,1,1), seasonal_order=(1,1,1,4))\n",
    "        result = model.fit(disp=False)\n",
    "        pred = result.get_prediction(start=serie.index[-1], end=serie.index[-1])\n",
    "        pred_value = pred.predicted_mean.iloc[0]\n",
    "        real_value = serie.iloc[-1]\n",
    "        sarimax_preds.append(pred_value)\n",
    "        sarimax_real.append(real_value)\n",
    "    except Exception as e:\n",
    "        continue  # Si falla el ajuste, lo salteamos\n",
    "\n",
    "sarimax_preds = np.array(sarimax_preds)\n",
    "sarimax_real = np.array(sarimax_real)\n",
    "sarimax_mape = np.mean(np.abs((sarimax_real - sarimax_preds) / sarimax_real)) * 100\n",
    "print(f\"MAPE SARIMAX (Error porcentual absoluto medio): {sarimax_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56472f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suma total de toneladas reales para diciembre 2019\n",
    "total_real_dic2019 = valores_reales['tn'].sum()\n",
    "\n",
    "# Suma total de toneladas predichas para diciembre 2019 (usar df_predicciones)\n",
    "total_pred_dic2019 = df_predicciones['pred_201912'].sum()\n",
    "\n",
    "print(f\"Suma total real de toneladas en diciembre 2019: {total_real_dic2019:.2f}\")\n",
    "print(f\"Suma total predicha de toneladas en diciembre 2019: {total_pred_dic2019:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción Seasonal Naive Forecast para diciembre 2019 (2019-12-01)\n",
    "# Usamos el valor de diciembre 2018 (2018-12-01) como predicción para diciembre 2019\n",
    "\n",
    "# Proyección lineal simple usando solo los valores de 2017-12-01 y 2018-12-01 para cada producto\n",
    "linear_simple_preds = []\n",
    "linear_simple_real = []\n",
    "\n",
    "for prod_id in pivot_df['product_id']:\n",
    "    serie = pivot_df[pivot_df['product_id'] == prod_id].iloc[0, 1:]\n",
    "    serie = serie.dropna()\n",
    "    # Convertir el índice a fechas\n",
    "    serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "    # Necesitamos valores para 2017-12-01, 2018-12-01 y 2019-12-01\n",
    "    if (\n",
    "        pd.Timestamp('2017-12-01') in serie.index and\n",
    "        pd.Timestamp('2018-12-01') in serie.index and\n",
    "        pd.Timestamp('2019-12-01') in serie.index\n",
    "    ):\n",
    "        x = np.array([201712, 201812]).reshape(-1, 1)\n",
    "        y = np.array([serie.loc[pd.Timestamp('2017-12-01')], serie.loc[pd.Timestamp('2018-12-01')]])\n",
    "        model = LinearRegression()\n",
    "        model.fit(x, y)\n",
    "        pred = model.predict(np.array([[201912]]))[0]\n",
    "        real = serie.loc[pd.Timestamp('2019-12-01')]\n",
    "        linear_simple_preds.append(pred)\n",
    "        linear_simple_real.append(real)\n",
    "\n",
    "linear_simple_preds = np.array(linear_simple_preds)\n",
    "linear_simple_real = np.array(linear_simple_real)\n",
    "linear_simple_mape = np.mean(np.abs((linear_simple_real - linear_simple_preds) / linear_simple_real)) * 100\n",
    "print(f\"MAPE Linear Simple (Error porcentual absoluto medio): {linear_simple_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22545ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La predicción para product_id=20001 en 2019-12 usando el modelo de regresión lineal ya fue calculada en la celda 16:\n",
    "# pred_201912 = model.predict(np.array([[201912]]))[0]\n",
    "\n",
    "print(linear_simple_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pmdarima.auto_arima: Esta biblioteca puede encontrar automáticamente los mejores órdenes (p,d,q)(P,D,Q,S) para cada serie individual,\n",
    "# basándose en criterios como AIC o BIC. Esto podría mejorar la precisión por producto, pero aumentaría el tiempo de cómputo.\n",
    "\n",
    "# Python\n",
    "\n",
    "# Ejemplo con pmdarima para el caso actual\n",
    "\n",
    "autoarima_preds = []\n",
    "autoarima_real = []\n",
    "\n",
    "for prod_id in pivot_df['product_id']:\n",
    "    serie = pivot_df[pivot_df['product_id'] == prod_id].iloc[0, 1:]\n",
    "    serie = serie.dropna()\n",
    "    # Necesitamos al menos 8 datos y valor real en 201912\n",
    "    if len(serie) < 8 or 201912 not in serie.index:\n",
    "        continue\n",
    "\n",
    "    # Convertir el índice a fechas\n",
    "    serie.index = pd.to_datetime(serie.index.astype(str), format='%Y%m')\n",
    "\n",
    "    # Usar todos los datos menos el último (2019-12-01) para entrenar\n",
    "    train_serie = serie.iloc[:-1]\n",
    "    try:\n",
    "        auto_model = pm.auto_arima(\n",
    "            train_serie,\n",
    "            start_p=1, start_q=1,\n",
    "            max_p=3, max_q=3, m=4,\n",
    "            start_P=0, seasonal=True,\n",
    "            d=1, D=1, trace=False,\n",
    "            error_action='ignore',\n",
    "            suppress_warnings=True,\n",
    "            stepwise=True\n",
    "        )\n",
    "        pred = auto_model.predict(n_periods=1)[0]\n",
    "        real = serie.iloc[-1]\n",
    "        autoarima_preds.append(pred)\n",
    "        autoarima_real.append(real)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "autoarima_preds = np.array(autoarima_preds)\n",
    "autoarima_real = np.array(autoarima_real)\n",
    "autoarima_mape = np.mean(np.abs((autoarima_real - autoarima_preds) / autoarima_real)) * 100\n",
    "print(f\"MAPE Auto-ARIMA (Error porcentual absoluto medio): {autoarima_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las predicciones y los valores reales del método seasonal naive\n",
    "df_seasonal_naive = pd.DataFrame({\n",
    "    'preds': autoarima_preds,\n",
    "    'real': autoarima_real\n",
    "})\n",
    "\n",
    "df_seasonal_naive.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
